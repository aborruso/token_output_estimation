# token_evaluate

A CLI utility to estimate the average token count for JSON output generated by LLM from structured text data.

## Overview

`token_evaluate` helps you estimate how many tokens an LLM will produce when extracting structured data from files (HTML, CSV, JSON, XML, etc.) and converting it to JSON format. This is particularly useful for:

- **Cost estimation**: Calculate processing costs before running large datasets
- **Performance planning**: Estimate token usage for batch operations
- **API quota management**: Plan token consumption for rate-limited APIs

## How it works

The tool:
1. Takes a structured data file and extraction instructions as input
2. Runs the LLM extraction **3 times** with random element selection
3. Counts tokens for each JSON output using `ttok`
4. Returns the **average token count** as a single integer

## Installation

### Prerequisites

You need to install these dependencies first:

1. **LLM CLI** - https://llm.datasette.io/
   ```bash
   pip install llm
   ```

2. **ttok** - Token counter utility
   ```bash
   pip install ttok
   ```

### Install token_evaluate

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd token_output_estimation
   ```

2. Install the utility:
   ```bash
   sudo make install
   ```

3. **Copy the template** to your LLM CLI templates directory:
   ```bash
   # Find your LLM templates directory
   llm templates path

   # Copy the template (replace <templates-path> with the actual path)
   cp resource/template.yml <templates-path>/token_output.yml
   ```

## Usage

### Basic usage
```bash
token_evaluate input_file "extraction instructions"
```

### Verbose mode (show JSON outputs and individual token counts)
```bash
token_evaluate -v input_file "extraction instructions"
```

### Examples

```bash
# Basic token count estimation
token_evaluate data.html "extract province and city"
# Output: 15

# Verbose mode to see the actual JSON outputs
token_evaluate -v data.csv "extract name, email and phone number"
# Output: Shows JSON for each run + final average

# Complex extraction
token_evaluate data.json "extract all available fields"
# Output: 89
```

## Supported Data Formats

- **HTML**: Structured markup with repeated elements
- **CSV**: Tabular data with columns
- **JSON**: Arrays of objects or structured data
- **XML**: Elements with repeated structures
- **Any structured text**: With repeated records/elements

## Template Configuration

The tool uses a specific LLM template (`token_output.yml`) that:
- Extracts only requested fields from structured data
- Always selects a random element (not the first)
- Returns clean JSON output only
- Adapts to different data formats automatically

## Output

- **Standard mode**: Single integer (average token count)
- **Verbose mode**: JSON outputs + token counts + final average
- **Pipe-friendly**: Only the number goes to stdout, diagnostics to stderr

## Rate Limiting

The tool automatically includes a 4-second delay between LLM calls to respect API rate limits (max 15 requests/minute).

## License

[Add your license information here]

## Contributing

[Add contribution guidelines here]
