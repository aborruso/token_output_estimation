# token_evaluate

A CLI utility to estimate the average token count for JSON output generated by LLM from structured text data.

## Overview

`token_evaluate` helps you estimate how many tokens an LLM will produce when extracting structured data from files (HTML, CSV, JSON, XML, etc.) and converting it to JSON format. This is particularly useful for:

- **Cost estimation**: Calculate processing costs before running large datasets
- **Performance planning**: Estimate token usage for batch operations
- **API quota management**: Plan token consumption for rate-limited APIs

## How it works

The tool:
1. Takes a structured data file and extraction instructions as input
2. Runs the LLM extraction **3 times** with random element selection
3. Counts tokens for each JSON output using `ttok`
4. Returns the **average token count** as a single integer

## Installation

### Prerequisites

You need to install these dependencies first:

1. **LLM CLI** - https://llm.datasette.io/
   ```bash
   pip install llm
   ```

2. **ttok** - Token counter utility
   ```bash
   pip install ttok
   ```

### Install token_evaluate

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd token_output_estimation
   ```

2. Install the utility:
   ```bash
   sudo make install
   ```

3. **Copy the template** to your LLM CLI templates directory:
   ```bash
   # Find your LLM templates directory
   llm templates path

   # Copy the template (replace <templates-path> with the actual path)
   cp resource/template.yml <templates-path>/token_output.yml
   ```

## Usage

### Basic usage
```bash
token_evaluate input_file "extraction instructions"
```

### Options

- `--verbose, -v`: Show JSON outputs and individual token counts for each run
- `--items, -n NUMBER`: Multiply token estimate by NUMBER items for total calculation
- `--model, -m MODEL`: Use specific LLM model (default: uses llm default)

### Examples

```bash
# Basic token count estimation
token_evaluate data.html "extract province and city"
# Output: {"single_item_tokens": 15}

# Verbose mode to see the actual JSON outputs
token_evaluate -v data.csv "extract name, email and phone number"
# Output: Shows JSON for each run + final average

# Use a specific model
token_evaluate -m gpt-4o data.html "extract province and city"
# Output: {"single_item_tokens": 15}

# Use model alias for faster/cheaper estimation
token_evaluate -m 4o-mini data.csv "extract name and email"
# Output: {"single_item_tokens": 10}

# Calculate total tokens for batch processing
token_evaluate -n 1000 data.json "extract all available fields"
# Output: {"single_item_tokens": 89, "items": 1000, "total_estimated_tokens": 89000}

# Combine options
token_evaluate -v -m gpt-4o -n 500 data.html "extract all fields"
# Output: Shows detailed run info + total estimation

# Pipe input with model selection
cat data.json | token_evaluate -m 4o-mini "extract key information"
# Output: {"single_item_tokens": 42}
```

## Supported Data Formats

- **HTML**: Structured markup with repeated elements
- **CSV**: Tabular data with columns
- **JSON**: Arrays of objects or structured data
- **XML**: Elements with repeated structures
- **Any structured text**: With repeated records/elements

## Template Configuration

The tool uses a specific LLM template (`token_output.yml`) that:
- Extracts only requested fields from structured data
- Always selects a random element (not the first)
- Returns clean JSON output only
- Adapts to different data formats automatically

**Important**: The template must be installed before first use. The tool will check for the template and provide installation instructions if missing.

## Output

The tool outputs JSON to stdout with the following structure:

### Single item estimation
```json
{"single_item_tokens": 42}
```

### Batch estimation (with --items)
```json
{"single_item_tokens": 42, "items": 1000, "total_estimated_tokens": 42000}
```

### Verbose mode
In verbose mode, diagnostics go to stderr:
- JSON output for each of the 3 runs
- Individual token counts
- Summary with calculated mean
- Final JSON result to stdout

## Model Selection

You can specify which LLM model to use with the `--model` or `-m` option:

```bash
# Use GPT-4o for higher quality extraction
token_evaluate -m gpt-4o data.html "extract fields"

# Use GPT-4o-mini for faster/cheaper estimation
token_evaluate -m 4o-mini data.html "extract fields"

# Use model aliases
token_evaluate -m 4o data.html "extract fields"  # Same as gpt-4o
```

If no model is specified, the tool uses your default LLM model (configured with `llm models default`).

## Rate Limiting

The tool automatically includes a 4-second delay between LLM calls to respect API rate limits (max 15 requests/minute).

## License

[Add your license information here]

## Contributing

[Add contribution guidelines here]
