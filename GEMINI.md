# Project Overview

This project provides a command-line utility named `token_evaluate` to estimate the average token count for JSON output generated by a Large Language Model (LLM) from structured text data (HTML, CSV, JSON, etc.). It is designed to help with cost estimation, performance planning, and API quota management.

The tool works by running an LLM extraction three times on a given input file with user-provided instructions. It then calculates the average token count of the resulting JSON outputs.

The core logic is implemented in a Bash script, `token_evaluate`, which orchestrates the process of calling the LLM, counting tokens, and calculating the average.

# Building and Running

## Prerequisites

- **LLM CLI**: `pip install llm`
- **ttok**: `pip install ttok`

## Installation

To install the `token_evaluate` utility, use the provided `Makefile`:

```bash
sudo make install
```

This command will copy the `token_evaluate` script to `/usr/local/bin`.

You also need to copy the LLM template:

```bash
# Find your LLM templates directory
llm templates path

# Copy the template (replace <templates-path> with the actual path)
cp resource/template.yml <templates-path>/token_output.yml
```

## Usage

To run the utility:

```bash
token_evaluate [options] <input_file> "<instructions>"
```

**Options:**

*   `-v`, `--verbose`: Show JSON outputs and individual token counts for each run.
*   `-n`, `--items <number>`: Specify the number of items to estimate the total token count for.

**Examples:**

```bash
# Basic token count estimation
token_evaluate sample/input_01.html "extract province and city"

# Verbose mode
token_evaluate -v sample/input_02.csv "extract name, email and phone number"
```

# Development Conventions

*   The main script `token_evaluate` is written in Bash.
*   The project uses a `Makefile` for simple installation and uninstallation.
*   The LLM interaction is defined in a YAML template (`resource/template.yml`). This template instructs the LLM to extract specific fields and return a clean JSON output.
*   The tool is designed to be pipe-friendly, with the final average token count sent to `stdout` and diagnostic information to `stderr`.
*   A 4-second delay is included between LLM calls to respect API rate limits.
