# PRD — token_evaluate CLI for JSON Output Token Estimation

## Title
Bash CLI `token_evaluate` to automatically estimate the average number of tokens needed to represent a single JSON item generated by an LLM from a structured HTML block

## Objective
Provide a simple and reusable CLI that fully automates the estimation of tokens produced by an LLM model when generating a structured JSON object from an HTML input block. The CLI directly returns an integer representing the average token estimate, facilitating integration into automation scripts and pipelines.

## Context
- Inputs consist of HTML blocks representing cards with structured data (e.g., province, city, center, address, contacts, hours).
- The desired output is a structured JSON object containing the key fields.
- The analysis serves to estimate **how many tokens are produced on average for each JSON item** generated by an LLM.

## Technology Stack
- **CLI `token_evaluate`** main bash script that orchestrates the process
- **LLM CLI** to execute the language model via YAML template
- **YAML Template `token_output`** to define output behavior and structure
- **`ttok` CLI** to measure the number of output tokens
- **Bash utilities** for data processing and average calculation

## Method

### CLI Syntax
```bash
token_evaluate [options] file_input "instructions on what to extract"
```

### Parameters
- **file_input**: HTML file containing the structured blocks to analyze
- **instructions**: string with specific instructions on which fields to extract (e.g., "extract province, city, center, address, contacts and hours")

### Options
- **--verbose, -v**: show JSON output and token count for each run
- **--items, -n NUMBER**: multiply the estimate by NUMBER items (useful for batch processing)
- **--model, -m MODEL**: specify which LLM model to use (default: model configured in llm)

### Internal Operation
1. **Validation**: Checks for dependencies (llm, ttok) and token_output template
   - If the template is missing, shows clear installation instructions
   - If the specified model doesn't exist, shows list of available models

2. **Processing**: The CLI internally executes the command:
   ```bash
   cat file_input | llm [-m model] -t token_output -p instructions "user_instructions" | ttok
   ```

3. **Repetition**: To ensure reliability, the CLI runs the process **3 times**

4. **Average Calculation**: Automatically calculates the average of tokens obtained from the 3 runs

5. **JSON Output**: Returns a JSON object with:
   - `single_item_tokens`: average estimate per single item
   - `items`: (optional) number of items specified with -n
   - `total_estimated_tokens`: (optional) total estimate if -n is specified

### Usage Examples

```bash
# Basic estimate
token_evaluate sample/input_01.html "extract province, city, center, address, contacts and hours"
# Output: {"single_item_tokens": 142}

# With specific model for higher quality
token_evaluate -m gpt-4o sample/input_01.html "extract province, city, center, address, contacts and hours"
# Output: {"single_item_tokens": 145}

# With economical model for quick tests
token_evaluate -m 4o-mini sample/input_01.html "extract province, city, center, address, contacts and hours"
# Output: {"single_item_tokens": 138}

# Estimate for batch of 1000 items
token_evaluate -n 1000 sample/input_01.html "extract province, city, center, address, contacts and hours"
# Output: {"single_item_tokens": 142, "items": 1000, "total_estimated_tokens": 142000}

# Verbose for debugging
token_evaluate -v -m gpt-4o sample/input_01.html "extract province, city, center, address, contacts and hours"
# Output: Shows JSON for each run + summary + final result
```

## Expected Output

- **JSON Format**: easily parsable structured output
  ```json
  {"single_item_tokens": 142}
  ```
- **Batch output**: with -n parameter includes total estimate
  ```json
  {"single_item_tokens": 142, "items": 1000, "total_estimated_tokens": 142000}
  ```
- **Pipe-friendly**: JSON to stdout, diagnostics to stderr
- **Reproducibility**: consistent results across multiple runs
- **Robust validation**: template and model checks with clear error messages

## Benefits

- **Ease of use**: intuitive CLI interface with only 2 parameters
- **Complete automation**: eliminates manual steps, automatic average calculation
- **Pipeline integration**: numeric output easily usable in bash scripts
- **Realistic measurement**: based on actual LLM model behavior
- **Precise cost estimation**: facilitates cost calculations (token × item × dataset size)
- **Reproducibility**: consistent and reliable results

## Future Extensions

- ~~**Multiple model support**: `-m` parameter to specify LLM model (`gpt-4`, `gpt-3.5-turbo`)~~ ✅ Implemented
- **Iteration configuration**: `-n` parameter to change number of repetitions (default: 3)
- ~~**Detailed output**: `-v` flag to show individual run results~~ ✅ Implemented
- **Custom templates**: `-t` parameter to specify templates other than `token_output`
- **Batch processing**: support for processing multiple files in parallel
- **Results export**: `--export` parameter to save results in CSV/JSON
- **Advanced statistics**: min/max/stddev in addition to average
